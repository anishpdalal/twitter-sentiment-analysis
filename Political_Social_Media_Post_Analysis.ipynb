{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip dataset\n",
    "zip_ref = zipfile.ZipFile('political-social-media-posts.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in dataset with Latin-1 encoding\n",
    "dataset = pd.read_csv('political_social_media.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "print(dataset.shape)\n",
    "# head\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level descriptions of fields we're interested in\n",
    "print(dataset.describe(include=[np.object])['message'])\n",
    "print(dataset.describe(include=[np.object])['source'])\n",
    "print(dataset.describe(include=[np.object])['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "fig1.set_size_inches(18.5, 10.5)\n",
    "plt.subplot(221)\n",
    "plt.title('Message Type')\n",
    "pd.value_counts(dataset['message']).plot.bar()\n",
    "plt.subplot(222)\n",
    "plt.title('Source')\n",
    "pd.value_counts(dataset['source']).plot.bar()\n",
    "plt.subplot(223)\n",
    "plt.title('Bias')\n",
    "pd.value_counts(dataset['bias']).plot.bar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neutral_messages = dataset[(dataset.bias == 'neutral')]\n",
    "partisan_messages = dataset[(dataset.bias == 'partisan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(2)\n",
    "\n",
    "fig2.set_size_inches(18.5, 10.5)\n",
    "plt.subplot(221)\n",
    "plt.title('Neutral Message Types')\n",
    "neutral_counts = pd.value_counts(neutral_messages['message'])\n",
    "neutral_counts.plot.bar()\n",
    "plt.subplot(222)\n",
    "plt.title('Partisan Message Types')\n",
    "partisan_counts = pd.value_counts(partisan_messages['message'])\n",
    "partisan_counts.plot.bar()\n",
    "plt.tight_layout()\n",
    "#########\n",
    "\n",
    "fig3, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig3.set_size_inches(18.5, 7.5)\n",
    "neutral_df = neutral_counts.to_frame(name=\"messages\")\n",
    "normalized_neutral_df = (neutral_df-neutral_df.min())/(neutral_df.max()-neutral_df.min())\n",
    "normalized_neutral_df.plot(kind='bar', ax=axes[0], sharex=False, sharey=False, title=\"Normalized Neutral Messages\")\n",
    "partisan_df = partisan_counts.to_frame(name=\"messages\")\n",
    "normalized_partisan_df = (partisan_df-partisan_df.min())/(partisan_df.max()-partisan_df.min())\n",
    "normalized_partisan_df.plot(ax=axes[1], kind='bar',sharex=False, sharey=False, title=\"Normalized Partisan Messages\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "idx = imdb.get_word_index()\n",
    "idx2word = {v: k for k, v in idx.items()}\n",
    "\n",
    "def one_hot_encoding(message):\n",
    "    return 0 if message == \"attack\" else 1\n",
    "\n",
    "def word_to_idx(words):\n",
    "    return [idx[word] if (word in idx and idx[word] <= 5000) else 5000 for word in words]\n",
    "\n",
    "attack_or_support_df = dataset[(dataset.message == 'attack') | (dataset.message == 'support')][['message','text']]\n",
    "attack_or_support_df = attack_or_support_df.reset_index(drop=True)\n",
    "attack_or_support_df['message'] = attack_or_support_df['message'].apply(one_hot_encoding)\n",
    "attack_or_support_df['text'] = attack_or_support_df['text'].apply(lambda x: word_to_idx(text_to_word_sequence(x)))\n",
    "attack_or_support_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_or_support_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [0]*len(attack_or_support_df['text'])\n",
    "for index, x in enumerate(attack_or_support_df['text']):\n",
    "    lengths[index] =len(x)\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(lengths), np.std(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_indices = attack_or_support_df.index[attack_or_support_df['message'] == 0].tolist()\n",
    "support_indices = attack_or_support_df.index[attack_or_support_df['message'] == 1].tolist()\n",
    "validation_indices = random.sample(range(len(attack_indices)), int(0.20*len(attack_indices))) + random.sample(range(len(support_indices)), int(0.20*len(support_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = attack_or_support_df.iloc[validation_indices]['message'].tolist()\n",
    "x_test = attack_or_support_df.iloc[validation_indices]['text'].tolist()\n",
    "y_train = attack_or_support_df[~attack_or_support_df.index.isin(validation_indices)]['message'].tolist()\n",
    "x_train = attack_or_support_df[~attack_or_support_df.index.isin(validation_indices)]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = sequence.pad_sequences(x_test, maxlen=100)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Embedding(5001, 32, input_length=100))\n",
    "mlp_model.add(Flatten())\n",
    "mlp_model.add(Dense(250, activation='relu'))\n",
    "mlp_model.add(Dense(1, activation='sigmoid'))\n",
    "mlp_model.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "print(mlp_model.summary())\n",
    "# Fit the model\n",
    "mlp_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "mlp_scores = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (mlp_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential()\n",
    "conv_model.add(Embedding(5001, 32, input_length=100))\n",
    "conv_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "conv_model.add(MaxPooling1D(pool_size=2))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(250, activation='relu'))\n",
    "conv_model.add(Dense(1, activation='sigmoid'))\n",
    "conv_model.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "print(conv_model.summary())\n",
    "conv_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "conv_scores = conv_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (conv_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cnn_model = Sequential()\n",
    "lstm_cnn_model.add(Embedding(5001, 32, input_length=100))\n",
    "lstm_cnn_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "lstm_cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "lstm_cnn_model.add(LSTM(100))\n",
    "lstm_cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_cnn_model.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm_cnn_model.summary())\n",
    "lstm_cnn_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)\n",
    "lstm_cnn_scores = lstm_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (lstm_cnn_scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
